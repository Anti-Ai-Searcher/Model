# =============================================================================
# detector/train_fixed.py
# -----------------------------------------------------------------------------
# ðŸ“Œ ëª©ì (purpose)
#   - "ì¸ê°„ vs AI(ìƒì„±í˜• ëª¨ë¸) í…ìŠ¤íŠ¸" ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ
#     ìŠ¤í¬ë¦½íŠ¸. WSL2 + RTX 3070(8GB) í™˜ê²½ì—ì„œ ì†ë„/ë©”ëª¨ë¦¬/ì•ˆì •ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ íŠœë‹.
#
# ðŸ“¦ ëª¨ë¸(Model)
#   - ê¸°ë³¸: RoBERTa-base (Transformer Encoder ê¸°ë°˜, bidirectional attention)
#     Â· ì‚¬ì „í•™ìŠµ(objective): Masked Language Modeling
#     Â· ë¯¸ì„¸ì¡°ì •(task): ì´ì§„ ë¶„ë¥˜(head: hidden_state â†’ [CLS] â†’ Linear(num_labels=2))
#   - ì„ íƒ: microsoft/deberta-v3-base (Disentangled self-attention; ë” ë¬´ê±°ìš¸ ìˆ˜ ìžˆìŒ)
#
# ðŸ§µ íŒŒì´í”„ë¼ì¸(Pipeline)
#   1) ë°ì´í„° ë¡œë”©: human(real) / ai(fake) jsonl (train/valid) â†’ í•©ì¹˜ê³  ë ˆì´ë¸” ë¶€ì—¬(0/1)
#   2) í† í¬ë‚˜ì´ì¦ˆ: HuggingFace Datasetsì˜ .map(num_proc=6) ë³‘ë ¬ ì²˜ë¦¬
#      Â· ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ ìºì‹œ(save_to_disk) â†’ ë™ì¼ ì„¤ì •ì´ë©´ ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì¦‰ì‹œ ë¡œë“œ
#   3) DataLoader: WSL2 ì•ˆì •ì„±ì„ ìœ„í•´ num_workers=0, pad_to_multiple_of=8 (AMP ìœ ë¦¬)
#   4) í•™ìŠµ: AMP(FP16) + TF32, Fused AdamW, SDPA(Flash/MemEfficient) ì„ í˜¸
#      Â· tqdmì— loss/acc/lr ì‹¤ì‹œê°„ í‘œì‹œ
#      Â· warmup + linear decay ìŠ¤ì¼€ì¤„ëŸ¬
#      Â· gradient clipping + early stopping
#   5) í‰ê°€: validì—ì„œ loss/accuracy/macro-F1 ì‚°ì¶œ â†’ ìµœê³  F1 ëª¨ë¸ ì €ìž¥
#
# âš™ï¸ ì„±ëŠ¥(throughput) íŒ
#   - L(ì‹œí€€ìŠ¤ ê¸¸ì´)â†“ â†’ ê³„ì‚°ëŸ‰ ~L^2ë¡œ ì¤„ì–´ë“¦. ê¸°ë³¸ L=96 (ì¶”ë¡ ì—ì„  ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë³´ì™„)
#   - AMP/TF32/SDPA/AdamW(fused) â†’ 1 epoch ~25~30ë¶„(500k ìƒ˜í”Œ, bs=64) ìˆ˜ì¤€ ê°€ëŠ¥
#   - ìºì‹œ ìžë™: ëª¨ë¸/ê¸¸ì´/ë°ì´í„° ì¡°í•©ë³„ë¡œ í´ë”ê°€ ë‹¬ë¼ì ¸ ì„žì´ì§€ ì•ŠìŒ
#
# ðŸ§ª ì‚¬ìš©ë²•
#   $ python detector/train_fixed.py
#   (í•„ìš”ì‹œ) --model, --max-seq-len, --batch-size, --epochs ë“± CLI ì¸ìžë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥
# =============================================================================

import argparse, os, random, time, re, contextlib               # ê¸°ë³¸ ìœ í‹¸ Â· ì»¨í…ìŠ¤íŠ¸
import numpy as np                                              # ë‚œìˆ˜ ê³ ì •/ê°„ë‹¨ ê³„ì‚°
import torch                                                    # PyTorch í•µì‹¬
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler  # ë¡œë”/ìƒ˜í”ŒëŸ¬
from torch.nn.attention import sdpa_kernel                      # âœ… ì‹ ê·œ SDPA ì»¨í…ìŠ¤íŠ¸
from transformers import (                                      # HF Transformers
    AutoTokenizer, AutoModelForSequenceClassification,
    DataCollatorWithPadding, get_linear_schedule_with_warmup
)
from datasets import (                                          # HF Datasets
    load_dataset, concatenate_datasets, DatasetDict, load_from_disk
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support # ì§€í‘œ
from torch.amp import GradScaler, autocast                      # AMP(ìžë™ í˜¼í•© ì •ë°€)
from tqdm import tqdm                                           # ì§„í–‰ë¥  ì¶œë ¥
from transformers.utils import logging as hf_logging            # HF ë¡œê·¸ ë ˆë²¨ ì œì–´
hf_logging.set_verbosity_error()                                # HF ê²½ê³  ê³¼ë‹¤ ì¶œë ¥ ë°©ì§€

# -------------------- ëŸ°íƒ€ìž„ íŠœë‹: í† í¬ë‚˜ì´ì €/TF32/cuDNN --------------------
os.environ["TOKENIZERS_PARALLELISM"] = "false"                  # í† í¬ë‚˜ì´ì € thread í­ì£¼ ë°©ì§€
torch.backends.cuda.matmul.allow_tf32 = True                    # TF32(ì•”íŽ˜ì–´â†‘) í—ˆìš© â†’ ì†ë„â†‘
torch.backends.cudnn.allow_tf32 = True                          # cuDNNì—ì„œë„ TF32 í—ˆìš©
torch.backends.cudnn.benchmark = True                           # ìž…ë ¥ í¬ê¸° ê³ ì • ì‹œ ì»¤ë„ auto-tune
try:
    torch.set_float32_matmul_precision("high")                  # PyTorch 2.x ê¶Œìž¥ ì„¤ì •
except Exception:
    pass

# ------------------------------ ìœ í‹¸ í•¨ìˆ˜ë“¤ ---------------------------------
def set_seed(seed: int = 42):
    """ì‹¤í—˜ ìž¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •."""
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def sanitize(s: str) -> str:
    """ê²½ë¡œ/íŒŒì¼ëª… ì•ˆì „ ë¬¸ìžì—´ë¡œ ì •ì œ."""
    s = s.replace('/', '_').replace(':', '_')
    return re.sub(r'[^a-zA-Z0-9_.-]+', '_', s)

def leaf_name(path_like: str) -> str:
    """'a/b/c' â†’ 'c' (ë§ˆì§€ë§‰ í† í°ë§Œ)"""
    return sanitize(path_like.strip('/').split('/')[-1])

def load_pair(prefix: str, split: str):
    """
    JSONL ë¡œë“œ: {prefix}.{split}.jsonl
    Â· HF datasetsê°€ ë‚´ë¶€ ìºì‹œë¥¼ í™œìš©í•´ ë¹ ë¥´ê²Œ ë¡œë“œ
    """
    return load_dataset('json', data_files=f"{prefix}.{split}.jsonl",
                        split='train', cache_dir='./cache')

def cache_path(args) -> str:
    """
    í† í¬ë‚˜ì´ì¦ˆ ê²°ê³¼ë¥¼ ì €ìž¥/ë¡œë“œí•  ë””ìŠ¤í¬ ê²½ë¡œ ìƒì„±.
    data_tag='auto'ë©´ (real_leaf__fake_leaf__v1) íŒ¨í„´ìœ¼ë¡œ ìžë™ íƒœê¹….
    """
    model_tag = sanitize(args.model)
    if args.data_tag is None or args.data_tag.lower() == 'auto':
        real_tag = leaf_name(args.real_dataset)                 # ì˜ˆ: 'webtext'
        fake_tag = leaf_name(args.fake_dataset)                 # ì˜ˆ: 'gemini'
        auto_tag = f"{real_tag}__{fake_tag}__v1"               # í•„ìš”ì‹œ --data-tag ë¡œ v2, v3â€¦
    else:
        auto_tag = sanitize(args.data_tag)
    # ì˜ˆ: ./cache/tok/roberta-base__L96__webtext__gemini__v1
    return os.path.join(args.tok_cache_root,
                        f"{model_tag}__L{args.max_seq_len}__{auto_tag}")

# -------------------------- ë°ì´í„°ì…‹ ë¹Œë“œ & ìºì‹œ ----------------------------
def build_tokenized_datasets(args, tokenizer):
    """
    1) ìºì‹œê°€ ìžˆìœ¼ë©´ load_from_disk
    2) ì—†ìœ¼ë©´ raw jsonl ë¡œë“œ â†’ ë¼ë²¨ë§ â†’ ë³‘í•© â†’ (ì„ íƒì )ì„œë¸Œìƒ˜í”Œ â†’ í† í¬ë‚˜ì´ì¦ˆ(map)
       â†’ save_to_disk â†’ ë°˜í™˜
    """
    keep = ['input_ids', 'attention_mask', 'labels']            # í›ˆë ¨ì— í•„ìš”í•œ ì—´ë§Œ ìœ ì§€
    path = cache_path(args)                                     # ìºì‹œ ê²½ë¡œ ê³„ì‚°

    if os.path.exists(path):                                    # 1) ìºì‹œ ížˆíŠ¸
        print(f"[cache] loading pretokenized datasets from: {path}")
        dd = load_from_disk(path)                               # ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
        dd = DatasetDict({k: v.with_format('torch') for k, v in dd.items()})  # í…ì„œ í¬ë§·
        return dd

    # 2) ìºì‹œ ì—†ìŒ â†’ ì›ë³¸ ë¡œë“œ í›„ ì „ì²˜ë¦¬
    print("[cache] not found. Building tokenized datasets...")

    # splitë³„ jsonl ì½ê³  ë¼ë²¨ ë¶€ì—¬: human=0, ai=1
    real_train = load_pair(os.path.join(args.data_dir, args.real_dataset), 'train').map(lambda x: {'labels': 0})
    fake_train = load_pair(os.path.join(args.data_dir, args.fake_dataset), 'train').map(lambda x: {'labels': 1})
    real_val   = load_pair(os.path.join(args.data_dir, args.real_dataset), 'valid').map(lambda x: {'labels': 0})
    fake_val   = load_pair(os.path.join(args.data_dir, args.fake_dataset), 'valid').map(lambda x: {'labels': 1})

    # ì‚¬ëžŒ/AI í•©ì¹˜ê¸°(ìˆœì„œëŠ” samplerê°€ ì„žì–´ì¤Œ)
    train = concatenate_datasets([real_train, fake_train])
    valid = concatenate_datasets([real_val,   fake_val])

    # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ìƒ˜í”Œ ìˆ˜ ì œí•œ(0ì´ë©´ ì „ì²´)
    if args.limit_train > 0:
        train = train.select(range(min(args.limit_train, len(train))))
    if args.limit_valid > 0:
        valid = valid.select(range(min(args.limit_valid, len(valid))))

    # ë©€í‹°í”„ë¡œì„¸ì‹± í† í¬ë‚˜ì´ì¦ˆ(WSL2ì—ì„œë„ datasets.mapì˜ num_procì€ ìž˜ ìž‘ë™)
    nproc = args.num_proc if args.num_proc > 0 else max(1, (os.cpu_count() or 1) // 2)
    print(f"[tokenize] num_proc={nproc}, max_len={args.max_seq_len}")

    # í† í¬ë‚˜ì´ì € ì½œë°±: truncation=True + max_length=ì§€ì •
    def tok(ex): return tokenizer(ex['text'], truncation=True, max_length=args.max_seq_len)

    # ì‹¤ì œ í† í¬ë‚˜ì´ì¦ˆ: ì—´ ì •ë¦¬ í›„ torch í…ì„œ í¬ë§·ìœ¼ë¡œ ë°”ê¿ˆ
    train = train.map(tok, batched=True, num_proc=nproc,
                      remove_columns=[c for c in train.column_names if c not in keep]
                     ).with_format('torch')
    valid = valid.map(tok, batched=True, num_proc=nproc,
                      remove_columns=[c for c in valid.column_names if c not in keep]
                     ).with_format('torch')

    # ë””ìŠ¤í¬ì— ì €ìž¥(ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì¦‰ì‹œ ë¡œë“œ)
    dd = DatasetDict({'train': train, 'valid': valid})
    os.makedirs(os.path.dirname(path), exist_ok=True)
    dd.save_to_disk(path)
    print(f"[cache] saved pretokenized datasets to: {path}")
    return dd

def build_loaders(dd, tokenizer, args, device):
    """
    í† í¬ë‚˜ì´ì¦ˆëœ DatasetDict â†’ DataLoader ìƒì„±.
    pad_to_multiple_of=8: AMP(FP16)ì—ì„œ í…ì„œ ì–¼ë¼ì¸ë¨¼íŠ¸ë¡œ ì»¤ë„ íš¨ìœ¨ ìƒìŠ¹.
    """
    collator = DataCollatorWithPadding(tokenizer,
                                       pad_to_multiple_of=8 if device == 'cuda' else None)
    train_ds, valid_ds = dd['train'], dd['valid']

    # í•™ìŠµì€ RandomSampler(ì…”í”Œ), ê²€ì¦ì€ SequentialSampler(ìˆœì°¨)
    train_loader = DataLoader(train_ds, sampler=RandomSampler(train_ds),
                              batch_size=args.batch_size, collate_fn=collator,
                              pin_memory=True, num_workers=args.num_workers,
                              persistent_workers=False)          # WSL2: workers=0 ì•ˆì „

    valid_loader = DataLoader(valid_ds, sampler=SequentialSampler(valid_ds),
                              batch_size=args.eval_batch_size, collate_fn=collator,
                              pin_memory=True, num_workers=args.num_workers,
                              persistent_workers=False)

    print(f"[data] train={len(train_ds):,}  valid={len(valid_ds):,}  "
          f"steps/epoch={len(train_loader):,} (bs={args.batch_size})")
    return train_loader, valid_loader

# ------------------------------ í‰ê°€ ë£¨í‹´ -----------------------------------
@torch.no_grad()
def evaluate(model, loader, device):
    """
    ê²€ì¦ ë£¨í”„: loss/acc/macro-F1 ì‚°ì¶œ.
    @torch.no_grad(): í‰ê°€ ì¤‘ ê·¸ëž˜í”„Â·ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”(ì†ë„â†‘).
    """
    model.eval()                                                # ë“œë¡­ì•„ì›ƒ/ì •ê·œí™” ê³ ì •
    total_loss = 0.0; n = 0                                     # ëˆ„ì  ì†ì‹¤/ìƒ˜í”Œ ìˆ˜
    all_preds, all_labels = [], []                              # ì˜ˆì¸¡/ì •ë‹µ ë²„í¼

    for batch in tqdm(loader, total=len(loader), desc="Valid", ncols=120, leave=False):
        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}  # Hâ†’D ë³µì‚¬ ê²¹ì¹˜ê¸°
        out = model(**batch)                                    # forward
        loss, logits = out.loss, out.logits                     # ì†ì‹¤Â·ë¡œì§“
        bsz = batch['labels'].size(0)
        total_loss += loss.item() * bsz; n += bsz               # í‰ê·  ì†ì‹¤ ê³„ì‚°ìš©
        all_preds.extend(logits.argmax(-1).detach().cpu().tolist())   # ì˜ˆì¸¡ ë¼ë²¨
        all_labels.extend(batch['labels'].detach().cpu().tolist())    # ì •ë‹µ ë¼ë²¨

    acc = accuracy_score(all_labels, all_preds)                 # ì •í™•ë„
    p, r, f1, _ = precision_recall_fscore_support(              # ë§¤í¬ë¡œ P/R/F1
        all_labels, all_preds, average='macro', zero_division=0
    )
    return {'loss': total_loss / max(n, 1), 'acc': acc, 'precision': p, 'recall': r, 'f1': f1}

# ------------------------------ í•™ìŠµ ë£¨í‹´ -----------------------------------
def train(args):
    set_seed(args.seed)                                         # ìž¬í˜„ì„±
    device = 'cuda' if torch.cuda.is_available() else 'cpu'     # ìž¥ì¹˜ ì„ íƒ
    print("Device:", device)

    # SDPA(Flash/MemEfficient) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    # - ì‹ ê·œ API(torch.nn.attention.sdpa_kernel)ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ,
    #   í•´ë‹¹ with ë¸”ë¡ì—ì„œë§Œ ì»¤ë„ ìš°ì„ ìˆœìœ„ê°€ ì ìš©ë¨.
    sdpa_ctx = sdpa_kernel(enable_flash=bool(args.flash_sdp),
                           enable_mem_efficient=bool(args.flash_sdp),
                           enable_math=not bool(args.flash_sdp)) if device == 'cuda' else contextlib.nullcontext()

    # í† í¬ë‚˜ì´ì €/ëª¨ë¸ ë¡œë“œ (num_labels=2 â†’ ì´ì§„ ë¶„ë¥˜ í—¤ë“œê°€ ë¶™ì€ ìƒíƒœ)
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
    model = AutoModelForSequenceClassification.from_pretrained(args.model, num_labels=2).to(device)

    # ë°ì´í„°ì…‹(ìºì‹œ ìžë™) ìƒì„± â†’ DataLoader
    dd = build_tokenized_datasets(args, tokenizer)
    train_loader, valid_loader = build_loaders(dd, tokenizer, args, device)

    # ì˜µí‹°ë§ˆì´ì €: Fused AdamW(ê°€ëŠ¥ ì‹œ) â†’ GPU step ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    use_fused = torch.cuda.is_available()
    try:
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,
                                      weight_decay=args.weight_decay, eps=1e-8,
                                      fused=use_fused)
        if use_fused:
            print("AdamW fused=True")
    except TypeError:
        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,
                                      weight_decay=args.weight_decay, eps=1e-8)

    # ìŠ¤ì¼€ì¤„ëŸ¬: linear decay + warmup
    steps_total = args.epochs * len(train_loader)
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                int(args.warmup_ratio * steps_total),
                                                steps_total)
    scaler = GradScaler(enabled=(device == 'cuda'))            # AMP ìŠ¤ì¼€ì¼ëŸ¬

    best_f1 = -1.0; patience = args.patience                   # Early stopping ìƒíƒœ
    with sdpa_ctx:                                             # âœ… í•™ìŠµÂ·í‰ê°€ ì „ì²´ êµ¬ê°„ì— SDPA ì ìš©
        if args.flash_sdp and device == 'cuda':
            print("SDPA: flash/mem_efficient enabled")

        for epoch in range(1, args.epochs + 1):                # ì—í­ ë£¨í”„
            model.train()                                      # í•™ìŠµ ëª¨ë“œ
            running_loss = 0.0; t0 = time.time()               # ë¡œê¹…ìš© ëˆ„ì ê°’
            running_correct = 0; running_seen = 0              # ì§„í–‰ ì¤‘ ì •í™•ë„ ì§‘ê³„
            bar = tqdm(train_loader, total=len(train_loader),
                      desc=f"Train e{epoch}", ncols=120)       # ì§„í–‰ë¥  ë°”

            for step, batch in enumerate(bar, 1):              # ë¯¸ë‹ˆë°°ì¹˜ ë£¨í”„
                # ë¹„ë™ê¸° ìž¥ì¹˜ ì´ë™(non_blocking)ìœ¼ë¡œ Hâ†’D ë³µì‚¬ì™€ ì—°ì‚° ê²¹ì¹˜ê¸°
                batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

                # AMP: Tensor Core/FP16 ê²½ë¡œ ì‚¬ìš© (ì†ë„â†‘, ë©”ëª¨ë¦¬â†“)
                with autocast(device_type=device, enabled=(device == 'cuda')):
                    out = model(**batch)                       # forward
                    loss = out.loss / args.grad_accum          # grad accumulation ëŒ€ë¹„ ìŠ¤ì¼€ì¼

                    # ì§„í–‰ ì¤‘ ì •í™•ë„ ê³„ì‚°(ê°€ë²¼ì›€, ê·¸ëž˜í”„ ë¶„ë¦¬)
                    preds = out.logits.detach().argmax(dim=-1)
                    bsz = batch['labels'].size(0)
                    running_correct += (preds == batch['labels']).sum().item()
                    running_seen += bsz

                # ìŠ¤ì¼€ì¼ëœ ê·¸ë¼ë””ì–¸íŠ¸ ì—­ì „íŒŒ
                scaler.scale(loss).backward()

                # grad_accum ìŠ¤í…ë§ˆë‹¤ ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
                if step % args.grad_accum == 0:
                    scaler.unscale_(optimizer)                 # clip ì „ì— unscale
                    torch.nn.utils.clip_grad_norm_(model.parameters(),
                                                   args.max_grad_norm)
                    scaler.step(optimizer); scaler.update()    # ì˜µí‹°ë§ˆì´ì € ìŠ¤í… + ìŠ¤ì¼€ì¼ ì—…ë°ì´íŠ¸
                    optimizer.zero_grad(set_to_none=True)      # grad ë©”ëª¨ë¦¬ í•´ì œ
                    scheduler.step()                           # ëŸ¬ë‹ë ˆì´íŠ¸ ìŠ¤ì¼€ì¤„

                running_loss += loss.item() * args.grad_accum  # ì›ëž˜ ìŠ¤ì¼€ì¼ë¡œ ëˆ„ì 

                # ë¡œê·¸ ê°±ì‹ (ë„ˆë¬´ ìžì£¼ ê°±ì‹ í•˜ë©´ ì˜¤ë²„í—¤ë“œ â†’ log_every ê°„ê²©ìœ¼ë¡œ)
                if step % args.log_every == 0 or step == 1:
                    train_acc = running_correct / max(running_seen, 1)
                    bar.set_postfix(loss=f"{running_loss/step:.4f}",
                                    acc=f"{train_acc:.4f}",
                                    lr=f"{scheduler.get_last_lr()[0]:.2e}")

            # ì—í­ ì¢…ë£Œ â†’ ê²€ì¦
            val = evaluate(model, valid_loader, device)
            mins = (time.time() - t0) / 60.0
            print(f"[Epoch {epoch}] train_loss={running_loss/len(train_loader):.4f} | "
                  f"val_loss={val['loss']:.4f} val_acc={val['acc']:.4f} val_f1={val['f1']:.4f} | "
                  f"time={mins:.1f}m")

            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ìž¥(ê¸°ì¤€: macro-F1)
            if val['f1'] > best_f1 + 1e-4:
                best_f1 = val['f1']; patience = args.patience
                os.makedirs(os.path.dirname(args.ckpt), exist_ok=True)
                torch.save({'state_dict': model.state_dict(), 'model_name': args.model}, args.ckpt)
                print(f"â˜… Saved best model to {args.ckpt} (f1={best_f1:.4f}, model={args.model})")
            else:
                patience -= 1
                if patience <= 0:
                    print("Early stopping."); break            # ì¡°ê¸° ì¢…ë£Œ

# --------------------------- ê¸°ë³¸ ì¸ìž(Default) -----------------------------
def build_argparser():
    ap = argparse.ArgumentParser()

    # ë°ì´í„° ë£¨íŠ¸ & íŒŒì¼ prefix (jsonl ê²½ë¡œ ì ‘ë‘ì‚¬)
    ap.add_argument('--data-dir', default='data')               # ë£¨íŠ¸ í´ë”
    ap.add_argument('--real-dataset', default='human_data/webtext')     # ì‚¬ëžŒ ë°ì´í„° prefix
    ap.add_argument('--fake-dataset', default='ai_data/gemini/gemini')  # AI ë°ì´í„° prefix

    # ìµœì í™”ëœ ê¸°ë³¸ê°’(ì†ë„Â·ì•ˆì •ì„± ìš°ì„ )
    ap.add_argument('--model', default='roberta-base')          # í•„ìš”ì‹œ deberta-v3-baseë¡œ ë¹„êµ
    ap.add_argument('--max-seq-len', type=int, default=96)      # Lâ†“ â†’ ì†ë„â†‘(ì¶”ë¡ ì€ ìœˆë„ìš°ë¡œ ë³´ì™„)
    ap.add_argument('--batch-size', type=int, default=64)       # 3070 8GB ê¶Œìž¥ê°’
    ap.add_argument('--eval-batch-size', type=int, default=64)
    ap.add_argument('--epochs', type=int, default=50)           # early stoppingì´ ìžˆìœ¼ë‹ˆ ê³¼í•¨ OK
    ap.add_argument('--lr', type=float, default=2e-5)           # AdamW ê¸°ë³¸ í•™ìŠµë¥ 
    ap.add_argument('--weight-decay', type=float, default=0.01) # L2 ì •ê·œí™”
    ap.add_argument('--warmup-ratio', type=float, default=0.1)  # 10% ì›Œë°ì—…
    ap.add_argument('--grad-accum', type=int, default=1)        # ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ 1 ìœ ì§€
    ap.add_argument('--max-grad-norm', type=float, default=1.0) # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦½
    ap.add_argument('--ckpt', default='./logs/best-model.pt')   # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    ap.add_argument('--patience', type=int, default=5)          # F1 ê°œì„  ì—†ì„ ë•Œ í—ˆìš© ì—í­ ìˆ˜
    ap.add_argument('--seed', type=int, default=42)             # ìž¬í˜„ì„±

    # WSL2 ì•ˆì „ êµ¬ë™
    ap.add_argument('--num-workers', type=int, default=0)       # DataLoader í”„ë¡œì„¸ìŠ¤(WSLì€ 0 ê¶Œìž¥)
    ap.add_argument('--log-every', type=int, default=50)        # tqdm ì—…ë°ì´íŠ¸ ê°„ê²©

    # í† í¬ë‚˜ì´ì¦ˆ ìºì‹œ(ìžë™ ì‚¬ìš©) ì„¤ì •
    ap.add_argument('--num-proc', type=int, default=6)          # datasets.map ë³‘ë ¬ í† í¬ë‚˜ì´ì¦ˆ
    ap.add_argument('--tok-cache-root', default='./cache/tok')  # ìºì‹œ ë£¨íŠ¸ í´ë”
    ap.add_argument('--data-tag', default='auto')               # auto â†’ ì¡°í•©ë³„ í´ë” ìžë™ ìƒì„±
    ap.add_argument('--limit-train', type=int, default=0)       # ë¹ ë¥¸ ì‹¤í—˜ìš© ì„œë¸Œì…‹(0=ì „ì²´)
    ap.add_argument('--limit-valid', type=int, default=0)

    # SDPA(Flash/MemEfficient) ì‚¬ìš©(ê°€ëŠ¥ ì‹œ)
    ap.add_argument('--flash-sdp', action='store_true', default=True)
    return ap

# --------------------------------- ì§„ìž…ì  -----------------------------------
if __name__ == "__main__":
    args = build_argparser().parse_args()                       # ì¸ìž íŒŒì‹±(ê¸°ë³¸ê°’ ìœ„ì£¼)
    train(args)                                                 # í•™ìŠµ ì‹œìž‘